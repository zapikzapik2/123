Нам необходимо настроить вектор весов $\vec {w}$ – его произведение с вектором признаков $\vec {x}$ будет определять, классифицируем ли мы каждый анамнез как "пациент с диабетом второго типа" или же нет. 

Функция потерь измеряет, насколько плохо наша модель справляется с задачей, сравнивая получаемые ответы с реальными данными. Наша задача — сократить значение этой функции.

[Градиентный спуск](https://ru.wikipedia.org/wiki/%D0%93%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA) – алгоритм оптимизации, позволяющий моделям машинного обучения сходиться к минимальным значениям при помощи повторяющихся шагов.

[Градиент](https://ru.wikipedia.org/wiki/%D0%93%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82) (или же производная) показывает уклон функции потерь. Таким образом, для минимизации потерь, нам необходимо двигаться в сторону, противоположную увеличению градиента. На изображении приведен пример градиента для двух весов $w_0$ и $w_1$. Один из его минимумов достигается при $w_0 = 0$ и $w_1 = 0.5$. На каждой итерации алгоритм будет подбирать значения весов, находящиеся все ближе к этому минимуму, двигаясь вдоль красной стрелки.

<img src="gradient-descent.png" class="center" width="75%"/>

Вот как выглядит простой алгоритм градиентного спуска:
- Задать изначальные значения весов $\vec{w}$ случайным образом.

- Вычислить функцию стоимости: $$Q(\vec{w}) = \sum\limits_{i \in \text{training set}} L(M_i(\vec{w}))$$
  где $L(M_i(\vec{w}))$ – функция потерь, $M_i(\vec{w}) = \langle \vec{w}, \vec{x}_i\rangle y_i$ – [отступ](http://www.machinelearning.ru/wiki/index.php?title=%D0%9B%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D1%8B%D0%B9_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80#.D0.9F.D0.BE.D0.BD.D1.8F.D1.82.D0.B8.D0.B5_.D0.BE.D1.82.D1.81.D1.82.D1.83.D0.BF.D0.B0), величина, характеризующая, насколько близко объект находится к границе двух классов. Чем меньше отступ, тем больше вероятность ошибки при классификации. Отступ отрицателен тогда и только тогда, когда алгоритм допускает ошибку на данном объекте.

- Вычислить градиенты $\bigtriangledown Q(\vec{w})$ функции стоимости:
$\bigtriangledown Q(\vec{w}) = (\frac{\partial Q(\vec{w})}{\partial w_j} )_{j=0}^n
= \sum \mathcal{L}'(\langle \vec{w}, \vec{x}_i \rangle y_i) \vec{x}_i y_i$

- Обновить значения весов пропорционально значению антиградиента (ведь мы движемся в сторону уменьшения потерь), т.е.
$$\vec{w} = \vec{w} - \alpha \bigtriangledown Q(\vec{w})$$
Здесь $\alpha$ – темп обучения, определяющий размер шагов в направлении антиградиента. Чем он меньше, тем меньше вероятность ошибки, но взамен придется пожертвовать скоростью работы алгоритма.
- Повторять до тех пор, пока $Q(w)$ не перестанет уменьшаться, или же не будет достигнуто иное заранее заданное условие останова.

Алгоритм будет сравнивать данные из диагнозов и настраивать вектор весов так, чтобы с каждым шагом было правильно угадано как можно большее количество имеющих или не имеющих заболевание.


Стоит с большой осторожностью настраивать параметр $\alpha$ (темп обучения) – слишком большие значения $\alpha$ приведут к тому, что мы "перескочим" минимум, маленькие же приведут к медленной работе алгоритма.


### Задание

Реализуйте метод градиентного спуска для обучения линейного классификатора в виде класса `GradientDescent`. Он находится в файле `gradient_descent.py`.

Метод `fit` должен вернуть список значений функций стоимости $Q(w)$ на каждой итерации градиентного спуска.

В качестве критерия остановки следует использовать отсечку `threshold` для расстояния между векторами весов на текущей и
предыдущей итерациях. Расстояние можно выбрать любое, например, Евклидово или `l1`.

В файле `precision_recall` находятся функции для вычисления точности и полноты. Их можно использовать для оценки качества классификации. 

<div class="hint">
Точность определяет то, насколько можно доверять классификатору. В нашем случае точность будет определяться отношением количества анамнезов, для которых <b>верно</b> предсказано наличие диабета 2 типа, к суммарному количеству анамнезов, для которых оно предсказано. Чем выше точность алгоритма, тем реже он будет ошибаться в классификации элементов.

Полнота показывает количество элементов класса, которые алгоритм сумел верно к нему отнести. В нашем случае это отношение количества анамнезов, которые алгоритм верно отнес к имеющим диабет, к реальному количеству больных диабетом в выборке. Чем больше полнота алгоритма, тем реже он упускает элементы, которые должны были попасть в тот или иной класс.
</div>

В файле `utils.py` находится функция `train_test_split`, разбивающая заданную выборку на обучающую и контрольную. Также там находится функция `plot_classification`, которая строит график зависимости величины ошибки классификатора от итерации. Если импортировать эту функцию в task.py, вызвать ее в main и запустить скрипт, то на нем можно наглядно наблюдать, с какой скоростью изменяется ошибка в зависимости от темпа обучения и функции потерь. Графики для обеих функций будут сохранены как изображения. Как уже упоминалось ранее, градиентный спуск при маленьких значениях темпа обучения сходится довольно медленно, так что будьте готовы к ожиданию результатов работы.

В данный момент `plot_classification` строит по три графика для каждой из функций потерь – с темпами обучения $1e-6$, $1e-5$ и $1e-4$ соответственно. Как уже упоминалось выше, при больших $\alpha$ существует вероятность "перескочить минимум" – алгоритм на одном шаге еще не достигнет условия останова, а на следующем уже начнет отдаляться от искомых значений. Это вызовет дальнейшее увеличение градиента и отступов, что в итоге приведет к проблеме ["взрывающихся градиентов"](https://www.machinelearningmastery.ru/exploding-gradients-in-neural-networks/). Этот эффект можно наблюдать, если установить $\alpha = 1e-3$. Отступы и градиенты будут увеличиваться после прохождения минимума, пока не вызовут переполнение при возведении в степень в функции потерь и все значения не станут неопределенными. У этой проблемы есть ряд решений, которые мы не будем рассматривать в данном курсе, так как она решится во время модификации алгоритма в следующем задании. 

В данном задании может быть полезна функция [numpy.linalg.norm](https://numpy.org/doc/1.18/reference/generated/numpy.linalg.norm.html), вычисляющая нормы матриц.
