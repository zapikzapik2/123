Проблема предыдущего алгоритма заключается в том, что, чтобы определить новое приближение вектора весов, необходимо вычислить градиент от каждого элемента выборки, что может сильно замедлять алгоритм. Стохастический градиентный спуск (**SGD**) вычисляет градиент для каждого обновления, используя лишь один обучающий объект $x_i$ (выбранный случайно).
Идея подобных вычислений в том, что градиент, вычисленный таким образом, является [стохастическим](https://ru.wikipedia.org/wiki/%D0%A1%D0%BB%D1%83%D1%87%D0%B0%D0%B9%D0%BD%D1%8B%D0%B9_%D0%BF%D1%80%D0%BE%D1%86%D0%B5%D1%81%D1%81) приближением градиента, вычисленного на всей обучающей выборке.
Каждое обновление, таким образом, можно вычислить гораздо быстрее, чем при групповом градиентном спуске, а по совокупности многих обновлений алгоритм будет двигаться в основном в том же направлении.

В стохастическом градиентном спуске на основе небольшого количества образцов (**Stochastic mini-batch gradient descent**) вычисляется градиент для каждой небольшой группы объектов из тренировочных данных.

Для начала обучающая выборка делится на небольшие группы (к примеру, по `k` элементов). Обновление происходит для каждой подобной группы. В зависимости от задачи, `k` обычно принимает значения от 30 до 500.
### Задание

Реализуйте метод стохастического градиентного спуска для обучения линейного классификатора.

Как и в случае с `GradientDescent`, метод `fit` должен
возвращать значения функции стоимости $Q$ на каждой итерации.
Чтобы оценить $Q$ на каждой итерации, воспользуйтесь формулой
$$Q = (1 − \eta)Q + \eta L_i$$

где $L_i$ – это значение функции потерь на данной итерации , а значение $\eta \in [0, 1]$, используемое для вычисления оценки $Q$, можно выбрать любое, например,
`1 / len(X)`. Так как величина $Q$ не стабильна, использовать
её для определения сходимости не следует. Как мы могли убедиться в предыдущем задании, при неправильном подборе параметров результат достигнут не будет.

Вместо этого предлагается использовать "стратегию оптимиста": сделать ровно `n_iter`
итераций и надеяться, что за это время стохастический градиентный спуск
сойдётся.

Параметр `k` определяет размер случайной подвыборки из
`X`, используемой для вычисления градиента.

В файле `stohastic_gradient-descent.py` представлена заготовка реализации. Обратите внимание, что функция для вычисления градиента вынесена отдельно (`calc_grad(self, w, X, y)`), добавлен параметр $\eta$, а главное – в функции `fit` цикл `while` уступил место `for i in range(self.n_iter)`. Это решает проблему неустойчивости `Q`, однако может приводить к худшему приближению результатов. Подобный компромисс времени работы, надежности и устойчивости нередко встречается при настройке алгоритмов машинного обучения.

Для визуализации результата необходимо модифицировать функцию `plot_classification(X, y)`:
```python
X_train, y_train, X_test, y_test = train_test_split(X, y, 0.8)

    n_iter = 5000

    for loss in [sigmoid_loss, log_loss]:
        for k in [1, 10, 50]:
            plt.clf()
            for alpha, color in zip([1e-6, 1e-4, 1e-3, 1e-2, 1e-1, 1],
                                    ["red", "blue", "green", "magenta", "yellow", "cyan"]):
                gd = StochasticGradientDescent(alpha=alpha, k=k, n_iter=n_iter)
```
Здесь добавлено количество итераций `n_iter = 5000` и еще один внутренний цикл, чтобы посмотреть результаты работы с `k = 1`, `k = 10`, `k = 50`. Для сохранения изображений с каждым рассматриваемым размером подвыборки нужно также модифицировать строки с заданием имен изображений:
```python
plt.title("SGD({}, k={})".format(loss.__name__, k))
plt.legend()
plt.savefig("sdg-{}-{}.png".format(loss.__name__, k))
```

Помимо этого, необходимо модифицировать несколько строк в других фалах:

*utils.py*:
```python
from stochastic_gradient_descent import StochasticGradientDescent
```
*task.py*:
```python
from utils import plot_classification
```

`main` *в task.py*:
```python
plot_classification(X, y)
```

Затем запустите `task.py`. Визуализации работы алгоритма для различных `k` с использованием различных `loss` функций появятся слева в *Course View*.

Попробуйте изменять параметры `n_iter` и `k` и посмотреть, как будут изменяться ошибки алгоритма и время его работы.

Результатом работы алгоритма будет преднастроенная система с вектором весов, позволяющая с определенной вероятностью предугадать по данным анамнеза, может ли человек быть болен диабетом 2 типа. Точность системы зависит от параметров, при которых происходила настройка.