В данном уроке мы рассмотрели пример реализации линейного классификатора с использованием стохастического градиентного спуска.

Этот метод — наиболее простой в реализации из всех методов локальной оптимизации, однако он является мощным инструментом для решения подобного класса задач. Мы рассмотрели пример классификации для двух классов – болен ли человек диабетом 2 типа или нет. Однако, метод может применяться и для большего количества классов. В таком случае, алгоритм потребует изрядной модификации. 

Для решения реальных задач нет необходимости воспроизводить алгоритм вручную – он реализован, к примеру, в библиотеке [scikit](https://scikit-learn.org/stable/modules/sgd.html). При использовании данной версии необходимо будет передавать функции параметры, разобранные в данном уроке, такие как количество итераций для "стратегии оптимиста" (`n_iter`) или же вид функции потерь (`loss`). Помимо прочего, алгоритм, реализованный в *scikit*, уже имеет готовые настройки для борьбы с взрывающимися градиентами.

Метод стохастического градиентного спуска может быть использован для настройки [метода опорных векторов (Support Vector Machine)](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%BE%D0%BF%D0%BE%D1%80%D0%BD%D1%8B%D1%85_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BE%D0%B2) или же иных линейных классификаторов и регрессоров. Нередко он используется для классификации текста или же работы с естественным языком (Natural Language Processing). Также метод используется при настройке однослойного перцептрона и [нейронных сетей](http://www.machinelearning.ru/wiki/index.php?title=%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C). Это будет подробнее рассмотрено далее в курсе.